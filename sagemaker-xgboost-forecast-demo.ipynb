{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime, date, timedelta\n",
    "from io import StringIO\n",
    "\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from data import passwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/image/design.png\" width=900 height=900 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "**We will build a forecast model to predict the sales of energy drinks for a retail store located in Iowa City,Iowa. We have 1 year historical data, dated back from 2021-06-01 to 2022-07-04. We will use 2021-06-01 to 2022-06-19 as training date and then predict the demand for the dates between 2022-06-20 to 2022-07-04.**\n",
    "\n",
    "**We use suggested radius of 1.76 km to search for the nearby events to this store. There are 23 venues close to this store within 1.76km radius. By running the Beam and category importance model, we learned that there are six event categories having statistical correlation to the demand which are:**\n",
    "- Sports\n",
    "- Public holidays\n",
    "- School holidays\n",
    "- Expos\n",
    "- Obervances\n",
    "- Severe weather\n",
    "- Concerts\n",
    "- Performing arts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Severe weather events with Demand Impact Patterns and Polygons\n",
    "\n",
    "* Severe Weather is one of the most impactful event categories in the world. Warnings or alerts of severe weather may lead to disruption and can have a huge influence on demand.\n",
    "* Severe weather events impact demand before and after an event. PredictHQâ€™s Demand Impact Pattern accurately captures the leading, lagging and coincident effects of a severe weather event on demand. \n",
    "* Easy to use severe weather event features in a forecast through feature API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/image/swdip.png\" width=900 height=900 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_important_results = [\n",
    "    \"public_holidays\",\n",
    "    \"sports\",\n",
    "    \"school_holidays\",\n",
    "    \"expos\",\n",
    "    \"observances\",\n",
    "    \"sw\",\n",
    "    \"concerts\",\n",
    "    \"performing_arts\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_FORMAT = \"%Y-%m-%d\"\n",
    "FEATURES_API_URL = \"https://api.predicthq.com/v1/features\"\n",
    "ACCESS_TOKEN = passwords.ACCESS_TOKEN\n",
    "SEVERE_WEATHER_FEATURES = {\n",
    "    \"phq_impact_severe_weather_air_quality_retail\",\n",
    "    \"phq_impact_severe_weather_blizzard_retail\",\n",
    "    \"phq_impact_severe_weather_cold_wave_retail\",\n",
    "    \"phq_impact_severe_weather_cold_wave_snow_retail\",\n",
    "    \"phq_impact_severe_weather_cold_wave_storm_retail\",\n",
    "    \"phq_impact_severe_weather_dust_retail\",\n",
    "    \"phq_impact_severe_weather_dust_storm_retail\",\n",
    "    \"phq_impact_severe_weather_flood_retail\",\n",
    "    \"phq_impact_severe_weather_heat_wave_retail\",\n",
    "    \"phq_impact_severe_weather_hurricane_retail\",\n",
    "    \"phq_impact_severe_weather_thunderstorm_retail\",\n",
    "    \"phq_impact_severe_weather_tornado_retail\",\n",
    "    \"phq_impact_severe_weather_tropical_storm_retail\",\n",
    "}\n",
    "SCHOOL_HOLIDAYS_FEATURE = \"phq_attendance_school_holidays\"\n",
    "\n",
    "\n",
    "def get_features_api_severe_weather_events(lat, lon, start, end, rank_threshold=30):\n",
    "    start = datetime.strptime(start, DATE_FORMAT).date()\n",
    "    end = datetime.strptime(end, DATE_FORMAT).date()\n",
    "\n",
    "    print(\"Querying Features API...\")\n",
    "    result = []\n",
    "    for gte, lte in get_date_groups(start, end):\n",
    "        print(f\"{gte} -> {lte}\")\n",
    "        request_data = {\n",
    "            \"location\": {\"geo\": {\"lat\": lat, \"lon\": lon, \"radius\": \"1m\"}},\n",
    "            \"active\": {\"gte\": gte, \"lte\": lte},\n",
    "        }\n",
    "        for feature in SEVERE_WEATHER_FEATURES:\n",
    "            request_data[feature] = {\n",
    "                \"stats\": [\"max\"],\n",
    "                \"phq_rank\": {\"gte\": rank_threshold},\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{FEATURES_API_URL}\",\n",
    "                headers={\"Authorization\": f\"Bearer {ACCESS_TOKEN}\"},\n",
    "                json=request_data,\n",
    "            ).json()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return {}, f\"{e}\"\n",
    "\n",
    "        for day in response[\"results\"]:\n",
    "            features = {\"date\": day[\"date\"]}\n",
    "            features.update(\n",
    "                {f: day[f][\"stats\"][\"max\"] for f in SEVERE_WEATHER_FEATURES}\n",
    "            )\n",
    "            result.append(features)\n",
    "    return result, None\n",
    "\n",
    "\n",
    "def get_date_groups(start, end):\n",
    "    \"\"\"\n",
    "    Features API allows range up to 90 days, so we have to do several requests\n",
    "    \"\"\"\n",
    "\n",
    "    def _split_dates(s, e):\n",
    "        capacity = timedelta(days=90)\n",
    "        interval = 1 + int((e - s) / capacity)\n",
    "        for i in range(interval):\n",
    "            yield s + capacity * i\n",
    "        yield e\n",
    "\n",
    "    dates = list(_split_dates(start, end))\n",
    "    for i, (d1, d2) in enumerate(zip(dates, dates[1:])):\n",
    "        if d2 != dates[-1]:\n",
    "            d2 -= timedelta(days=1)\n",
    "        yield d1.strftime(DATE_FORMAT), d2.strftime(DATE_FORMAT)\n",
    "\n",
    "\n",
    "res = get_features_api_severe_weather_events(\n",
    "    41.6576029, -91.53717840355998, \"2021-06-01\", \"2022-07-04\", 60\n",
    ")\n",
    "df_severe_weather_features = pd.DataFrame(res[0])\n",
    "columns_constant = [\n",
    "    col\n",
    "    for col in df_severe_weather_features.sum()[1:].to_dict().keys()\n",
    "    if df_severe_weather_features[col].sum() == 0\n",
    "]\n",
    "df_severe_weather_features.drop(columns=columns_constant, inplace=True)\n",
    "df_severe_weather_features.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTENDED_FEATURES = [\n",
    "    \"phq_attendance_community\",\n",
    "    \"phq_attendance_concerts\",\n",
    "    \"phq_attendance_conferences\",\n",
    "    \"phq_attendance_expos\",\n",
    "    \"phq_attendance_festivals\",\n",
    "    \"phq_attendance_performing_arts\",\n",
    "    \"phq_attendance_sports\",\n",
    "    \"phq_attendance_school_holidays\",\n",
    "]\n",
    "HOLIDAY_FEATURES = [\n",
    "    \"phq_rank_observances\",\n",
    "    \"phq_rank_public_holidays\",\n",
    "]\n",
    "\n",
    "\n",
    "def get_features_api_data(lat, lon, start, end, radius, rank_threshold=30):\n",
    "    start = datetime.strptime(start, DATE_FORMAT).date()\n",
    "    end = datetime.strptime(end, DATE_FORMAT).date()\n",
    "\n",
    "    print(\"Querying Features API...\")\n",
    "    result = []\n",
    "    for gte, lte in get_date_groups(start, end):\n",
    "        print(f\"{gte} -> {lte}\")\n",
    "        request_data = {\n",
    "            \"location\": {\"geo\": {\"lat\": lat, \"lon\": lon, \"radius\": f\"{radius}m\"}},\n",
    "            \"active\": {\"gte\": gte, \"lte\": lte},\n",
    "        }\n",
    "        for feature in ATTENDED_FEATURES:\n",
    "            request_data[feature] = {\n",
    "                \"stats\": [\"sum\"],\n",
    "                \"phq_rank\": {\"gte\": rank_threshold},\n",
    "            }\n",
    "\n",
    "        for feature in HOLIDAY_FEATURES:\n",
    "            request_data[feature] = True\n",
    "\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{FEATURES_API_URL}\",\n",
    "                headers={\"Authorization\": f\"Bearer {ACCESS_TOKEN}\"},\n",
    "                json=request_data,\n",
    "            ).json()\n",
    "        except Exception as e:\n",
    "            return {}, f\"{e}\"\n",
    "\n",
    "        for day in response[\"results\"]:\n",
    "            # print(day)\n",
    "            features = {\"date\": day[\"date\"]}\n",
    "            features.update({f: day[f][\"stats\"][\"sum\"] for f in ATTENDED_FEATURES})\n",
    "            features.update(\n",
    "                {f: sum(day[f][\"rank_levels\"].values()) for f in HOLIDAY_FEATURES}\n",
    "            )\n",
    "            result.append(features)\n",
    "    return result, None\n",
    "\n",
    "\n",
    "res = get_features_api_data(\n",
    "    41.6576029, -91.53717840355998, \"2021-06-01\", \"2022-07-30\", 1760, 30\n",
    ")\n",
    "df_attended_holidays = pd.DataFrame(res[0])\n",
    "columns_constant = [\n",
    "    col\n",
    "    for col in df_attended_holidays.columns[1:]\n",
    "    if col.replace(\"phq_attendance_\", \"\").replace(\"phq_rank_\", \"\")\n",
    "    not in category_important_results\n",
    "]\n",
    "df_attended_holidays.drop(columns=columns_constant, inplace=True)\n",
    "df_attended_holidays.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"sw\" in category_important_results:\n",
    "    df_event_features = df_attended_holidays.merge(\n",
    "        df_severe_weather_features, on=\"date\", how=\"left\"\n",
    "    )\n",
    "else:\n",
    "    df_event_features = df_attended_holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load demand and event feature through csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load demand dataset and event features\n",
    "df_demand = pd.read_csv(\"data/demand.csv\")\n",
    "df_demand[\"date\"] = pd.to_datetime(df_demand[\"date\"])\n",
    "df_event_features[\"date\"] = pd.to_datetime(df_event_features[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine event features with time trend features\n",
    "#### (3 layers: day of week, week of year, month of year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date to time relevant feature\n",
    "df_event_features[[\"day_of_week\", \"week_of_year\", \"month_of_year\"]] = (\n",
    "    df_event_features[\"date\"]\n",
    "    .map(lambda x: [x.day_of_week, x.weekofyear, x.month])\n",
    "    .to_list()\n",
    ")\n",
    "df = df_demand.merge(df_event_features, how=\"left\", on=\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a forecast using XGBoost model based on the above features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date_test = \"2022-06-20\"\n",
    "feature_columns = df.columns[2:].tolist()\n",
    "demand_column = \"demand\"\n",
    "\n",
    "X_train = df[df[\"date\"] < split_date_test][\n",
    "    [demand_column] + feature_columns\n",
    "]  # for SageMaker XGBoost training input, the first column is target\n",
    "X_test = df[df[\"date\"] >= split_date_test][feature_columns]\n",
    "y_test = df[df[\"date\"] >= split_date_test][demand_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data management\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"demo-xgb-demand-forecasting\"\n",
    "output_path = f\"s3://{bucket}/{prefix}/demand_forecast/output\"\n",
    "train_path = f\"s3://{bucket}/{prefix}/demand_forecast/train.csv\"\n",
    "\n",
    "# upload training data\n",
    "X_train.to_csv(train_path, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize hyperparameters\n",
    "# https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst#parameters-for-tree-booster\n",
    "hyperparameters = {\n",
    "    \"learning_rate\": \"0.1\",\n",
    "    \"max_depth\": \"6\",\n",
    "    \"objective\": \"reg:squarederror\",  # default\n",
    "    \"seed\": \"42\",\n",
    "    \"num_round\": \"100\",  # equivalent to n_estimators\n",
    "    \"verbosity\": \"1\",\n",
    "}\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "region = \"us-west-2\"\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.5-1\")\n",
    "\n",
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "xgb_model = sagemaker.estimator.Estimator(\n",
    "    image_uri=xgboost_container,\n",
    "    hyperparameters=hyperparameters,\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    volume_size=1,  # 1 GB\n",
    "    output_path=output_path,\n",
    ")\n",
    "\n",
    "# define the data type and paths to the training and validation datasets\n",
    "train_input = TrainingInput(train_path, content_type=\"text/csv\")\n",
    "\n",
    "# execute the XGBoost training job\n",
    "xgb_model.fit({\"train\": train_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb_model.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.t2.medium\", serializer=CSVSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = xgb_predictor.predict(X_test.values).decode(\"utf-8\")\n",
    "y_prediction = np.array([float(x) for x in response.split()])\n",
    "y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast the next two weeks' demand starting from 2022-06-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df[df[\"date\"] < split_date_test][\"date\"],\n",
    "        y=df[df[\"date\"] < split_date_test][demand_column],\n",
    "        name=\"y_training\",\n",
    "        mode=\"lines+markers\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df[df[\"date\"] >= split_date_test][\"date\"],\n",
    "        y=y_prediction,\n",
    "        name=\"y_prediction\",\n",
    "        mode=\"lines+markers\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df[df[\"date\"] >= split_date_test][\"date\"],\n",
    "        y=df[df[\"date\"] >= split_date_test][demand_column],\n",
    "        name=\"y_truth\",\n",
    "        mode=\"lines+markers\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/image/sports.png\" width=900 height=900 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the forecast with event features againest that without event features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_withoutevents = df[\n",
    "    [\"date\", \"demand\", \"day_of_week\", \"week_of_year\", \"month_of_year\"]\n",
    "]\n",
    "df_withoutevents.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns_withoutevents = df_withoutevents.columns[2:].tolist()\n",
    "X_train_withoutevents = df_withoutevents[df_withoutevents[\"date\"] < split_date_test][\n",
    "    [demand_column] + feature_columns_withoutevents\n",
    "]\n",
    "X_test_withoutevents = df_withoutevents[df_withoutevents[\"date\"] >= split_date_test][\n",
    "    feature_columns_withoutevents\n",
    "]\n",
    "y_test_withoutevents = df_withoutevents[df_withoutevents[\"date\"] >= split_date_test][\n",
    "    demand_column\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_withoutevents = (\n",
    "    f\"s3://{bucket}/{prefix}/demand_forecast/output_withoutevents\"\n",
    ")\n",
    "train_path_withoutevents = (\n",
    "    f\"s3://{bucket}/{prefix}/demand_forecast/train_withoutevents.csv\"\n",
    ")\n",
    "\n",
    "# upload training data\n",
    "X_train_withoutevents.to_csv(train_path_withoutevents, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize hyperparameters\n",
    "# https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst#parameters-for-tree-booster\n",
    "hyperparameters = {\n",
    "    \"learning_rate\": \"0.1\",\n",
    "    \"max_depth\": \"6\",\n",
    "    \"objective\": \"reg:squarederror\",  # default\n",
    "    \"seed\": \"42\",\n",
    "    \"num_round\": \"100\",  # equivalent to n_estimators\n",
    "    \"verbosity\": \"1\",\n",
    "}\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "region = \"us-west-2\"\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.5-1\")\n",
    "\n",
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "xgb_model_withoutevents = sagemaker.estimator.Estimator(\n",
    "    image_uri=xgboost_container,\n",
    "    hyperparameters=hyperparameters,\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    volume_size=1,  # 1 GB\n",
    "    output_path=output_path_withoutevents,\n",
    ")\n",
    "\n",
    "# define the data type and paths to the training and validation datasets\n",
    "train_input = TrainingInput(train_path_withoutevents, content_type=\"text/csv\")\n",
    "\n",
    "# execute the XGBoost training job\n",
    "xgb_model_withoutevents.fit({\"train\": train_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor_withoutevents = xgb_model_withoutevents.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.t2.medium\", serializer=CSVSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = xgb_predictor_withoutevents.predict(X_test_withoutevents.values).decode(\n",
    "    \"utf-8\"\n",
    ")\n",
    "y_prediction_withoutevents = np.array([float(x) for x in response.split()])\n",
    "y_prediction_withoutevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_withoutevents[df_withoutevents[\"date\"] < split_date_test][\"date\"],\n",
    "        y=df_withoutevents[df_withoutevents[\"date\"] < split_date_test][demand_column],\n",
    "        name=\"y_training\",\n",
    "        mode=\"lines+markers\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_withoutevents[df_withoutevents[\"date\"] >= split_date_test][\"date\"],\n",
    "        y=y_prediction_withoutevents,\n",
    "        name=\"y_prediction_no_events\",\n",
    "        mode=\"lines+markers\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_withoutevents[df_withoutevents[\"date\"] >= split_date_test][\"date\"],\n",
    "        y=df_withoutevents[df_withoutevents[\"date\"] >= split_date_test][demand_column],\n",
    "        name=\"y_truth\",\n",
    "        mode=\"lines+markers\",\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df[df[\"date\"] >= split_date_test][\"date\"],\n",
    "        y=y_prediction,\n",
    "        name=\"y_prediction_withevents\",\n",
    "        mode=\"lines+markers\",\n",
    "    )\n",
    ")\n",
    "# Display the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model comparison based on Mean Absolute Error (MAE) and Root Mean Square Error (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "MAE_model_withevents = mean_absolute_error(y_test, y_prediction)\n",
    "MAE_model_no_events = mean_absolute_error(\n",
    "    y_test_withoutevents, y_prediction_withoutevents\n",
    ")\n",
    "MAE_Model_improvement = (\n",
    "    (MAE_model_no_events - MAE_model_withevents) / MAE_model_no_events * 100\n",
    ")\n",
    "\n",
    "RMSE_model_withevents = mean_squared_error(y_test, y_prediction, squared=False)\n",
    "RMSE_model_no_events = mean_squared_error(\n",
    "    y_test_withoutevents, y_prediction_withoutevents, squared=False\n",
    ")\n",
    "RMSE_Model_improvement = (\n",
    "    (RMSE_model_no_events - RMSE_model_withevents) / RMSE_model_no_events * 100\n",
    ")\n",
    "\n",
    "print(f\"MAE for forecasting with events is {MAE_model_withevents:.2f}\")\n",
    "print(f\"MAE for forecasting without events is {MAE_model_no_events:.2f}\")\n",
    "print(\n",
    "    f\"MAE improvement of having event features in a forecast model is {MAE_Model_improvement:.2f}%\"\n",
    ")\n",
    "print(\" \")\n",
    "print(f\"RMSE for forecasting with events is {RMSE_model_withevents:.2f}\")\n",
    "print(f\"RMSE for forecasting without events is {RMSE_model_no_events:.2f}\")\n",
    "print(\n",
    "    f\"RMSE improvement of having event features in a forecast model is {RMSE_Model_improvement:.2f}%\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
